{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = \"catboost-ve\"\n",
    "# dataset = \"trauma_uk\"\n",
    "# results_file = \"results/subgroup_statistics_{model}_{dataset}.csv\".format(model=model, dataset=dataset)\n",
    "# df = pd.read_csv(results_file, index_col=\"Unnamed: 0\")\n",
    "# display(df)\n",
    "models = [\"NN-dropout\"]\n",
    "datasets = [\"trauma_uk\"]\n",
    "# models = [\"catboost-ve\", \"NN-dropout\"]\n",
    "# datasets = [\"diabetes\", \"trauma_uk\", \"ED_3day_readmit\", \"hospitalization_prediction\"] #\"critical_outcome\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test mean</th>\n",
       "      <th>test std</th>\n",
       "      <th>train mean</th>\n",
       "      <th>train std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>trauma_uk</th>\n",
       "      <td>5688.8</td>\n",
       "      <td>0.4</td>\n",
       "      <td>51199.2</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           test mean  test std  train mean  train std\n",
       "trauma_uk     5688.8       0.4     51199.2        0.4"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_dataset_sizes(dataset):\n",
    "    train_sizes = []\n",
    "    test_sizes = []\n",
    "    for i in range(10):\n",
    "        train = pd.read_csv(\"../data/cross_val_split/{0}_train_X_{1}.csv\".format(dataset,i))\n",
    "        test = pd.read_csv(\"../data/cross_val_split/{0}_test_X_{1}.csv\".format(dataset,i))\n",
    "        train_sizes.append(train.shape[0])\n",
    "        test_sizes.append(test.shape[0])\n",
    "    # train_size = np.mean(train_sizes)\n",
    "    # test_size = np.mean(test_sizes)\n",
    "    return {\"train_size\": train_sizes, \"test_size\": test_sizes}\n",
    "\n",
    "dataset_sizes = {}\n",
    "size_df = {}\n",
    "for dataset in datasets:\n",
    "    sze = get_dataset_sizes(dataset)\n",
    "    dataset_sizes[dataset] = sze\n",
    "    size_df[dataset] = {\"train mean\": np.mean(sze[\"train_size\"]),\n",
    "                        \"train std\": np.std(sze[\"train_size\"]),\n",
    "                        \"test mean\": np.mean(sze[\"test_size\"]),\n",
    "                        \"test std\": np.std(sze[\"test_size\"]),\n",
    "    }\n",
    "\n",
    "pd.DataFrame(size_df).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params\n",
    "\n",
    "def read_results_file(model, dataset, group):\n",
    "    results_file = \"results/{group}/subgroup_statistics_{model}_{dataset}.csv\".format(model=model, dataset=dataset, group=group)\n",
    "    # print(results_file)\n",
    "    df = pd.read_csv(results_file, index_col=\"Unnamed: 0\")\n",
    "    return df\n",
    "\n",
    "def get_sd_res(alpha_gain, n_bins, min_support, df):\n",
    "    cond_1 = (df[\"alpha_gain\"] == alpha_gain)\n",
    "    cond_2 = (df[\"n_bins\"] == n_bins)\n",
    "    cond_3 = (df[\"min_support\"] == min_support)\n",
    "    sub_df = df[cond_1 & cond_2 & cond_3]\n",
    "    return sub_df\n",
    "\n",
    "def format_res(mean, std):\n",
    "    mean = np.around(mean, 3)\n",
    "    std = np.around(std, 3)\n",
    "    return \"{0} ({1})\".format(mean, std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table Generation Functions\n",
    "Classification Accuracy\n",
    "| Dataset | No. Learned Rules | Accuracy | Coverage\n",
    "----------|-------------------|----------|----------\n",
    "\n",
    "2 Bin Accuracy\n",
    "| Dataset | Bin 0 Accuracy  | Bin 1 Accuracy |\n",
    "----------|-----------------|----------------|\n",
    "\n",
    "3 bin Accuracy\n",
    "| Dataset | Bin 0 Accuracy  | Bin 1 Accuracy | Bin 2 Accuracy \n",
    "----------|-----------------|----------------|----------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_subgroup_size(model, dataset, group, alpha_gain, n_bins, min_support):\n",
    "    df = read_results_file(model, dataset, group)\n",
    "    df = get_sd_res(alpha_gain, n_bins, min_support, df)\n",
    "    size_info = dataset_sizes[dataset][\"test_size\"]\n",
    "    print(size_info)\n",
    "    sizes = []\n",
    "    for id, row in df.iterrows():\n",
    "        n_g = int(row[\"no_rules\"])\n",
    "        iteration = int(row[\"iteration\"])\n",
    "        for g in range(n_g):\n",
    "            n_cond = row[\"subgroup {0} size\".format(g)]\n",
    "            if not np.isnan(n_cond):\n",
    "                sizes.append(n_cond/size_info[iteration])\n",
    "    sizes = np.asarray(sizes)\n",
    "\n",
    "    return np.mean(sizes), np.std(sizes)\n",
    "\n",
    "\n",
    "\n",
    "def generate_classification_accuracy_row(model, dataset, group, alpha_gain, n_bins, min_support):\n",
    "    df = read_results_file(model, dataset, group)\n",
    "    df = get_sd_res(alpha_gain, n_bins, min_support, df)\n",
    "    df = df[[\"dataset\", \"model\", \"no_rules\", \"bin assignment accuracy overall\", \"coverage\"]]\n",
    "    nr_mean = np.mean(df[\"no_rules\"])\n",
    "    nr_std = np.std(df[\"no_rules\"])\n",
    "    ba_mean = np.mean(df[\"bin assignment accuracy overall\"])\n",
    "    ba_std = np.std(df[\"bin assignment accuracy overall\"])\n",
    "    cv_mean = np.mean(df[\"coverage\"])\n",
    "    cv_std = np.std(df[\"coverage\"])\n",
    "    sz_mean, sz_std = calculate_average_subgroup_size(model, dataset, group, alpha_gain, n_bins, min_support)\n",
    "    to_return = {\n",
    "        \"dataset\": dataset,\n",
    "        \"model\": model,\n",
    "        \"no_rules\": nr_mean,\n",
    "        \"no_rules std\": nr_std,\n",
    "        \"bin assignment accuracy\": ba_mean,\n",
    "        \"bin assignment accuracy std\": ba_std,\n",
    "        \"coverage\": cv_mean,\n",
    "        \"coverage std\": cv_std,\n",
    "        \"size mean\": sz_mean, \n",
    "        \"size std\": sz_std \n",
    "    }\n",
    "    return to_return\n",
    "\n",
    "def generate_classification_accuracy_tables(alpha_gain, n_bins, min_support, group, model, datasets):\n",
    "        results = []\n",
    "        for dataset in datasets:\n",
    "            res = generate_classification_accuracy_row(model, dataset, group, alpha_gain, n_bins, min_support)\n",
    "            results.append(res)\n",
    "        results = pd.DataFrame(results)\n",
    "        # display(results)\n",
    "        return results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_nbin_accuracy_row(model, dataset, n_bins, group, alpha_gain, min_support):\n",
    "    # print(model, dataset, n_bins, group, alpha_gain, min_support)\n",
    "    key=\"pred\"\n",
    "    df = read_results_file(model, dataset, group)\n",
    "    df = get_sd_res(alpha_gain, n_bins, min_support, df)\n",
    "    cols = [\"dataset\", \"model\"]\n",
    "    for bin in range(n_bins):\n",
    "        cols.append(\"classsification accuracy {0} by {1}\".format(bin, key))\n",
    "    df = df[cols]\n",
    "    to_return = {\n",
    "        \"dataset\": dataset,\n",
    "        \"model\": model,\n",
    "    }\n",
    "    to_return = {\"dataset\": dataset, \"model\": model}\n",
    "    \n",
    "    for bin in range(n_bins):\n",
    "        bin_x_accuracy = np.mean(df[\"classsification accuracy {0} by {1}\".format(bin, key)])\n",
    "        bin_x_std = np.std(df[\"classsification accuracy {0} by {1}\".format(bin, key)])\n",
    "        # to_return[\"bin {0} accuracy\".format(bin)] = format_res(bin_x_accuracy, bin_x_std)\n",
    "        to_return[\"bin {0} accuracy\".format(bin)] = bin_x_accuracy\n",
    "        to_return[\"bin {0} accuracy std\".format(bin)] = bin_x_std\n",
    "    \n",
    "    diff_ranges = []\n",
    "    for bin in range(n_bins-1):\n",
    "        diff_ranges.append((bin, bin+1))\n",
    "    \n",
    "    for start, end in diff_ranges:\n",
    "        bin_start_accuracy = df[\"classsification accuracy {0} by {1}\".format(start, key)].values\n",
    "        bin_end_accuracy = df[\"classsification accuracy {0} by {1}\".format(end, key)].values\n",
    "        # print(np.unique(bin_start_accuracy, return_counts=True), np.unique(bin_end_accuracy, return_counts=True))\n",
    "        # print(np.count_nonzero(np.isnan(bin_start_accuracy)), np.count_nonzero(np.isnan(bin_start_accuracy)))\n",
    "        delta = bin_start_accuracy - bin_end_accuracy\n",
    "        # print(delta)\n",
    "        delta_mean = np.mean(delta)\n",
    "        delta_std = np.std(delta)\n",
    "        to_return[\"Delta {0}-{1}\".format(start, end)] = delta_mean #n, delta_std)#\"{0} ({1})\".format(delta_mean, delta_std)\n",
    "        to_return[\"Delta {0}-{1} std\".format(start, end)] = delta_std #format_res(delta_mean, delta_std)#\"{0} ({1})\".format(delta_mean, delta_std)\n",
    "\n",
    "    return to_return\n",
    "\n",
    "def generate_nbin_accuracy_accuracy_tables(alpha_gain, min_support, n_bins, group, model, datasets):\n",
    "    results = []\n",
    "    for dataset in datasets:\n",
    "        # print(model, dataset)\n",
    "        res = generate_nbin_accuracy_row(model, dataset, n_bins, group, alpha_gain, min_support)\n",
    "        results.append(res)\n",
    "    results = pd.DataFrame(results)\n",
    "    # display(results)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_workbook(alpha_gain, min_support, group, model, datasets):\n",
    "    wkbk = {}\n",
    "    wkbk[\"2 bin sd metrics\"] = generate_classification_accuracy_tables(alpha_gain=alpha_gain, n_bins=2, min_support=min_support, group=group, model=model, datasets=datasets)\n",
    "    wkbk[\"3 bin sd metrics\"] = generate_classification_accuracy_tables(alpha_gain=alpha_gain, n_bins=3, min_support=min_support, group=group, model=model, datasets=datasets)\n",
    "    # wkbk[\"4 bin sd metrics\"] = generate_classification_accuracy_tables(alpha_gain=alpha_gain, n_bins=4, min_support=min_support, group=group, model=model, datasets=datasets)\n",
    "    # wkbk[\"5 bin sd metrics\"] = generate_classification_accuracy_tables(alpha_gain=alpha_gain, n_bins=3, min_support=min_support, group=group, model=model, datasets=datasets)\n",
    "    wkbk[\"2 bin accuracy table\"] = generate_nbin_accuracy_accuracy_tables(alpha_gain=alpha_gain, n_bins=2, min_support=min_support, group=group, model=model, datasets=datasets)\n",
    "    # display(wkbk[\"2 bin accuracy table\"])\n",
    "    wkbk[\"3 bin accuracy table\"] = generate_nbin_accuracy_accuracy_tables(alpha_gain=alpha_gain, n_bins=3, min_support=min_support, group=group, model=model, datasets=datasets)\n",
    "    # display(wkbk[\"3 bin accuracy table\"])\n",
    "    # wkbk[\"4 bin accuracy table\"] = generate_nbin_accuracy_accuracy_tables(alpha_gain=alpha_gain, n_bins=4, min_support=min_support, group=group, model=model, datasets=datasets)\n",
    "    # wkbk[\"5 bin accuracy table\"] = generate_nbin_accuracy_accuracy_tables(alpha_gain=alpha_gain, n_bins=5, min_support=min_support, group=group, model=model, datasets=datasets)\n",
    "    xlsx_fn = \"results/{group}_{model}_alpha-gain{alpha_gain}_min-support{min_support}.xlsx\".format(group=group, alpha_gain=alpha_gain, min_support=min_support, model=model)\n",
    "    with pd.ExcelWriter(xlsx_fn) as writer:  \n",
    "        for key in wkbk:\n",
    "            df = wkbk[key]\n",
    "            df.to_excel(writer, sheet_name=key)  \n",
    "    return wkbk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code adapted from stack overflow\n",
    "\n",
    "def gen_df_plot(df, datasets, n_bins):\n",
    "    new_df = []\n",
    "    for dataset, dataset_full in datasets:\n",
    "        sub_df = df[df[\"dataset\"] == dataset]\n",
    "        \n",
    "        for index, row in sub_df.iterrows():\n",
    "            for bin in range(n_bins):\n",
    "                new_row = {}\n",
    "                new_row[\"Dataset\"] = dataset_full\n",
    "                new_row[\"bin\"] = \"Bin {0}\".format(bin)\n",
    "                new_row[\"Classification Accuracy\"] = row[\"bin {0} accuracy\".format(bin)]\n",
    "                new_row[\"std dev\"] = row[\"bin {0} accuracy std\".format(bin)]\n",
    "                new_df.append(new_row)\n",
    "    new_df = pd.DataFrame(new_df)\n",
    "    return new_df\n",
    "\n",
    "def grouped_barplot(df, cat, subcat, val, err,figname):\n",
    "    plt.figure(figsize=(12.4, 4.8)) \n",
    "    hatches = [\"/\", \"//\", \"///\"]\n",
    "    u = df[cat].unique()\n",
    "    x = np.arange(len(u))\n",
    "    subx = df[subcat].unique()\n",
    "    offsets = (np.arange(len(subx))-np.arange(len(subx)).mean())/(len(subx)+1.)\n",
    "    width= np.diff(offsets).mean()\n",
    "    for i,gr in enumerate(subx):\n",
    "        dfg = df[df[subcat] == gr]\n",
    "        plt.bar(x+offsets[i], \n",
    "                dfg[val].values, \n",
    "                width=width, \n",
    "                label=\"{}\".format(gr), \n",
    "                color = \"white\",\n",
    "                hatch = hatches[i],\n",
    "                edgecolor=\"black\",\n",
    "                yerr=None)\n",
    "    plt.xlabel(cat)\n",
    "    plt.ylabel(val)\n",
    "    plt.xticks(x, u)\n",
    "    plt.legend()\n",
    "    # plt.show()\n",
    "    plt.savefig(\"results/{group}/{figname}.png\".format(group=group,figname=figname))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "def calc_correlation_coef(x, y):\n",
    "    # print(x,y)\n",
    "    _, _, r_value, p_value, _ = scipy.stats.linregress(x, y)\n",
    "    return r_value, p_value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN-dropout\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['no_rules', 'bin assignment accuracy overall', 'coverage'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group, alpha_gain, min_support, model \u001b[38;5;129;01min\u001b[39;00m combos:\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# print(group, alpha_gain, min_support, model)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(model)\n\u001b[0;32m---> 18\u001b[0m     wkbk \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_workbook\u001b[49m\u001b[43m(\u001b[49m\u001b[43malpha_gain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_support\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[19], line 3\u001b[0m, in \u001b[0;36mgenerate_workbook\u001b[0;34m(alpha_gain, min_support, group, model, datasets)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_workbook\u001b[39m(alpha_gain, min_support, group, model, datasets):\n\u001b[1;32m      2\u001b[0m     wkbk \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m----> 3\u001b[0m     wkbk[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2 bin sd metrics\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_classification_accuracy_tables\u001b[49m\u001b[43m(\u001b[49m\u001b[43malpha_gain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha_gain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_bins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_support\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_support\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatasets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     wkbk[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3 bin sd metrics\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m generate_classification_accuracy_tables(alpha_gain\u001b[38;5;241m=\u001b[39malpha_gain, n_bins\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, min_support\u001b[38;5;241m=\u001b[39mmin_support, group\u001b[38;5;241m=\u001b[39mgroup, model\u001b[38;5;241m=\u001b[39mmodel, datasets\u001b[38;5;241m=\u001b[39mdatasets)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# wkbk[\"4 bin sd metrics\"] = generate_classification_accuracy_tables(alpha_gain=alpha_gain, n_bins=4, min_support=min_support, group=group, model=model, datasets=datasets)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# wkbk[\"5 bin sd metrics\"] = generate_classification_accuracy_tables(alpha_gain=alpha_gain, n_bins=3, min_support=min_support, group=group, model=model, datasets=datasets)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[17], line 48\u001b[0m, in \u001b[0;36mgenerate_classification_accuracy_tables\u001b[0;34m(alpha_gain, n_bins, min_support, group, model, datasets)\u001b[0m\n\u001b[1;32m     46\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[0;32m---> 48\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_classification_accuracy_row\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha_gain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_bins\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_support\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(res)\n\u001b[1;32m     50\u001b[0m results \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(results)\n",
      "Cell \u001b[0;32mIn[17], line 23\u001b[0m, in \u001b[0;36mgenerate_classification_accuracy_row\u001b[0;34m(model, dataset, group, alpha_gain, n_bins, min_support)\u001b[0m\n\u001b[1;32m     21\u001b[0m df \u001b[38;5;241m=\u001b[39m read_results_file(model, dataset, group)\n\u001b[1;32m     22\u001b[0m df \u001b[38;5;241m=\u001b[39m get_sd_res(alpha_gain, n_bins, min_support, df)\n\u001b[0;32m---> 23\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mno_rules\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbin assignment accuracy overall\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcoverage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     24\u001b[0m nr_mean \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_rules\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     25\u001b[0m nr_std \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstd(df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_rules\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/py38/lib/python3.8/site-packages/pandas/core/frame.py:3811\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3809\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   3810\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 3811\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   3814\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/py38/lib/python3.8/site-packages/pandas/core/indexes/base.py:6113\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6110\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6111\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6113\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6115\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6117\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/py38/lib/python3.8/site-packages/pandas/core/indexes/base.py:6176\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6173\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6175\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m-> 6176\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['no_rules', 'bin assignment accuracy overall', 'coverage'] not in index\""
     ]
    }
   ],
   "source": [
    "groups = [\"debug\" ]\n",
    "alpha_gains = [0,]\n",
    "min_support = [1]\n",
    "models = [\"NN-dropout\"]\n",
    "# models = [\"catboost-ve\", \"NN-dropout\"]\n",
    "datasets = [\"trauma_uk\"]\n",
    "# datasets = [\"trauma_uk\", \"diabetes\", \"critical_outcome\", \"ED_3day_readmit\", \"hospitalization_prediction\"]\n",
    "# datasets_plot = [(\"trauma_uk\", \"KY Trauma Triage\"), \n",
    "#                 (\"diabetes\", \"Pima Diabetes\"), \n",
    "#                 (\"critical_outcome\", \"Critical Outcome\"), \n",
    "#                 (\"ED_3day_readmit\", \"ED 3 Day Readmit\"), \n",
    "#                 (\"hospitalization_prediction\", \"Hosptialization Prediction\")]\n",
    "datasets_plot = [(\"trauma_uk\", \"KY Trauma Triage\")]\n",
    "combos = list(itertools.product(groups, alpha_gains, min_support, models))\n",
    "for group, alpha_gain, min_support, model in combos:\n",
    "    # print(group, alpha_gain, min_support, model)\n",
    "    print(model)\n",
    "    wkbk = generate_workbook(alpha_gain, min_support, group, model, datasets)\n",
    "    # r, p = calc_correlation_coef(x=wkbk[\"2 bin sd metrics\"][\"bin assignment accuracy\"], y=wkbk[\"2 bin accuracy table\"][\"Delta 0-1\"])\n",
    "    # print(\"2 bin assignment accuracy vs Delta 0-1: r^2 = {0}, p = {1}\".format(r, p))\n",
    "    # r, p = calc_correlation_coef(x=wkbk[\"3 bin sd metrics\"][\"bin assignment accuracy\"], y=wkbk[\"3 bin accuracy table\"][\"Delta 0-1\"])\n",
    "    # print(\"3 bin assignment accuracy vs Delta 0-1: r^2 = {0}, p = {1}\".format(r, p))\n",
    "    # r, p = calc_correlation_coef(x=wkbk[\"3 bin sd metrics\"][\"bin assignment accuracy\"], y=wkbk[\"3 bin accuracy table\"][\"Delta 1-2\"])\n",
    "    # print(\"3 bin assignment accuracy vs Delta 1-2: r^2 = {0}, p = {1}\".format(r, p))\n",
    "    # new_df = gen_df_plot(df=wkbk[\"2 bin accuracy table\"], datasets=datasets_plot, n_bins=2)\n",
    "    # grouped_barplot(df=new_df, cat = \"Dataset\", subcat = \"bin\", val = \"Classification Accuracy\", err=\"std dev\", figname=\"{0}_nbins{1}\".format(model, 2))\n",
    "    # new_df = gen_df_plot(df=wkbk[\"3 bin accuracy table\"], datasets=datasets_plot, n_bins=3)\n",
    "    # grouped_barplot(df=new_df, cat = \"Dataset\", subcat = \"bin\", val = \"Classification Accuracy\", err=\"std dev\", figname=\"{0}_nbins{1}\".format(model, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = [\"cut_final2\" ]\n",
    "alpha_gains = [0,]\n",
    "min_support = [1]\n",
    "models = [\"catboost-ve\", \"NN-dropout\"]\n",
    "datasets = [\"diabetes\", \"trauma_uk\", \"ED_3day_readmit\", \"hospitalization_prediction\"] #\"critical_outcome\",\n",
    "\n",
    "combos = list(itertools.product(groups, alpha_gains, min_support, models))\n",
    "for group, alpha_gain, min_support, model in combos:\n",
    "    try:\n",
    "        # print(group, alpha_gain, min_support, model)\n",
    "        generate_workbook(alpha_gain, min_support, group, model, datasets)\n",
    "    except:\n",
    "        print(\"ERROR: \", group, alpha_gain, min_support, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>train_size</th>\n",
       "      <th>test_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>trauma_uk</td>\n",
       "      <td>51199.2</td>\n",
       "      <td>5688.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>diabetes</td>\n",
       "      <td>691.2</td>\n",
       "      <td>76.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>critical_outcome</td>\n",
       "      <td>397293.3</td>\n",
       "      <td>44143.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ED_3day_readmit</td>\n",
       "      <td>397293.3</td>\n",
       "      <td>44143.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hospitalization_prediction</td>\n",
       "      <td>397293.3</td>\n",
       "      <td>44143.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      dataset  train_size  test_size\n",
       "0                   trauma_uk     51199.2     5688.8\n",
       "1                    diabetes       691.2       76.8\n",
       "2            critical_outcome    397293.3    44143.7\n",
       "3             ED_3day_readmit    397293.3    44143.7\n",
       "4  hospitalization_prediction    397293.3    44143.7"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "a8d4008d8d446fcac79315e60c8a20e2c2d23e177b36bb85454a7732dcd75a24"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
